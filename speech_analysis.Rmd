---
title: "From Obama to Trump: Changes in Presidential Language in the U.S.A. (2013–2025)"
author: "Pablo Aísa, Diego Fernández and Irantzu Lamarca"
date: "2025-04-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Trump's second investiture in the United States began just a few months ago and has revolutionized the world as we know it with his various economic and social measures. What has probably been most striking is the tone he has used to refer to other countries and their relationship with the United States. Trump's figure and his discourse were already known worldwide since he won the election in 2017, but the scene has been changing over the years. With his rapprochement with Russia in the Ukraine war and his direct confrontation with the European Union and China, Western relations in recent years have been weakened in recent months.

For this reason, we have decided to focus this paper on analyzing the speeches of recent presidents in the US Congress. In this way, we want to see what differences there are between the speeches of the three presidents (Donald Trump, Joe Biden and Barack Obama) and to see specifically how Donald Trump in 2025 differs from the rest.

It would indeed have been more appropriate to use the inaugural speeches that different presidents make once they have won the elections and are sworn in as such. However, these speeches are generally shorter. For this reason, we have chosen to select the first speeches made by each of the presidents in their presentation to Congress.

-   [Remarks by the President in the State of the Union Address, Obama 2013](https://obamawhitehouse.archives.gov/the-press-office/2013/02/12/remarks-President-state-union-address)
-   [Remarks by President Trump in Joint Address to Congress 2017](https://trumpwhitehouse.archives.gov/briefings-statements/remarks-president-trump-joint-address-congress/)
-   [Remarks by President Biden in Address to a Joint Session of Congress 2021](https://bidenwhitehouse.archives.gov/briefing-room/speeches-remarks/2021/04/29/remarks-by-president-biden-in-address-to-a-joint-session-of-congress/)
-   [Remarks by President Trump in Joint Address to Congress 2025](https://www.whitehouse.gov/remarks/2025/03/remarks-by-president-trump-in-joint-address-to-congress/)

In order to be able to analyze these texts as comprehensively as possible, we will use three text analysis techniques. The three techniques chosen are: *Sentiment Analysis*, *Term Frequency* and *Topic Modeling*. These tools will be used separately, but will be presented together at the end of the work in order to draw conclusive results on the different discourses. In particular, we are very interested in analysing the results of topic modelling, as it will be useful to group and complement the conclusions drawn from the first two techniques.

Here is the github link to the repository in order to be able to access the data documents (https://github.com/pbloas/speech_analysis)

## Libraries

Firstly, we load the libraries that will be used in this project.

```{r}
library(tidyverse)
library(readr)
library(tidytext)
library(tm)
library(topicmodels)
library(SnowballC)
library(textdata)
library(scales)
library(knitr)
library(kableExtra)
library(proxy)
library(wordcloud)
library(RColorBrewer)
library(reshape2)
```

We will use these libraries for:

-   Manipulate and clean up text (tidyverse, tidytext, readr, reshape2). To import, structure, and transform textual data into tidy formats that are easier to analyze.

-   Remove empty words and prepare textual data (tm, SnowballC, textdata). To clean and normalize the text by removing stopwords, applying stemming, and incorporating external lexicons.

-   Apply topic models such as LDA (topicmodels). To identify and analyze latent topics within a corpus of documents.

-   Measure similarity between documents (proxy). To compute distance or similarity metrics between text elements, which supports clustering or recommendation tasks.

-   Visualize and present results (wordcloud, RColorBrewer, scales, knitr, kableExtra). To create informative and aesthetic visualizations like word clouds and well-formatted tables for reports or presentations.

## Data cleaning

The first step will consist on obtaining the data and cleaning it to be able to further process the texts. The different speeches selected were prepared and transformed into a *.txt document* so they can be easily read in *Rstudio*.

### Trump 2025

We start with the last speech available which is the one that Trump made at the beginning of March 2025.

```{r}
# We remove the parts where the president was not talking
trump2025 <- read_lines("data/Trump_2025")

trump2025_filtered <- trump2025 %>%
  str_subset("^(AUDIENCE|SPEAKER|VICE PRESIDENT|REPRESENTATIVE GREEN|SPEAKER jOHNSON)", negate = TRUE)

# we remove the parenthesis
trump2025_filtered <- trump2025_filtered %>%
  str_replace_all("\\(.*?\\)", "")

# we remove "THE PRESIDENT:"
final_text_2025 <- str_replace(trump2025_filtered, "^THE PRESIDENT:\\s*", "")
# we remove empty rows
final_text_2025 <- final_text_2025[nchar(trimws(final_text_2025)) > 0]

```

Now we apply the same changes to the rest of the documents.

### Trump 2017

```{r}
# We remove the parts where the president was not talking
trump2017 <- read_lines("data/Trump_2017")

trump2017_filtered <- trump2017 %>%
  str_subset("^(AUDIENCE|SPEAKER|VICE PRESIDENT|REPRESENTATIVE GREEN|SPEAKER jOHNSON)", negate = TRUE)

# we remove the parenthesis
trump2017_filtered <- trump2017_filtered %>%
  str_replace_all("\\(.*?\\)", "")

# we remove "THE PRESIDENT:"
final_text_2017 <- str_replace(trump2017_filtered, "^THE PRESIDENT:\\s*", "")
# we remove empty rows
final_text_2017 <- final_text_2017[nchar(trimws(final_text_2017)) > 0]

```

### Biden 2021

```{r}
# We remove the parts where the president was not talking
biden2021 <- read_lines("data/Biden_2021")

biden2021_filtered <- biden2021 %>%
  str_subset("^(AUDIENCE|SPEAKER|VICE PRESIDENT|REPRESENTATIVE GREEN|SPEAKER jOHNSON)", negate = TRUE)

# we remove the parenthesis
biden2021_filtered <- biden2021_filtered %>%
  str_replace_all("\\(.*?\\)", "")

# we remove "THE PRESIDENT:"
final_text_2021 <- str_replace(biden2021_filtered, "^THE PRESIDENT:\\s*", "")
# we remove empty rows
final_text_2021 <- final_text_2021[nchar(trimws(final_text_2021)) > 0]
```

### Obama 2013

```{r}
# We remove the parts where the president was not talking
obama2013 <- read_lines("data/Obama_2013")

obama2013_filtered <- obama2013 %>%
  str_subset("^(AUDIENCE|SPEAKER|VICE PRESIDENT|REPRESENTATIVE GREEN|SPEAKER jOHNSON)", negate = TRUE)

# we remove the parenthesis
obama2013_filtered <- obama2013_filtered %>%
  str_replace_all("\\(.*?\\)", "")

# we remove "THE PRESIDENT:"
final_text_2013 <- str_replace(obama2013_filtered, "^THE PRESIDENT:\\s*", "")
# we remove empty rows
final_text_2013 <- final_text_2013[nchar(trimws(final_text_2013)) > 0]

```

## Text processing

Now, we will proceed with the three text analysis techniques that were previously mentioned (*Sentiment Analysis*, *Term Frequency* and *Topic Modeling*).

### Sentiment analysis

Sentiment analysis is a popular task in natural language processing. Our goal with sentiment analysis is to classify the text based on the mood or mentality expressed in the inauguration speeches. First, we will identify the most common joy and fear words in Trump´s speeches, then we will study their evolution and check the proportion of negative and positive words. We will finish the sentiment analysis with a *wordcloud* representation

Let's create a dataframe with all the text we want to analyze

```{r}
# Trump 2025
tidy_trump_2025 <- as.data.frame(final_text_2025) %>%
  mutate(
    speech = "Trump_2025",
    linenumber = row_number()
  )
tidy_trump_2025

# Trump 2017
tidy_trump_2017 <- as.data.frame(final_text_2017) %>%
  mutate(
    speech = "Trump_2017",
    linenumber = row_number()
  )
tidy_trump_2017

# Obama 2013
tidy_obama_2013 <- as.data.frame(final_text_2013) %>%
  mutate(
    speech = "Obama_2013",
    linenumber = row_number()
  )
tidy_obama_2013

# Biden 2021
tidy_biden_2021 <- as.data.frame(final_text_2021) %>%
  mutate(
    speech = "Biden_2021",
    linenumber = row_number()
  )
tidy_biden_2021

# tokenizing
tidy_trump_2025 <- tidy_trump_2025 %>%
  unnest_tokens(word, final_text_2025)

tidy_trump_2025

tidy_trump_2017 <- tidy_trump_2017 %>%
  unnest_tokens(word, final_text_2017)
tidy_trump_2017

tidy_obama_2013 <- tidy_obama_2013 %>%
  unnest_tokens(word, final_text_2013)
tidy_obama_2013

tidy_biden_2021 <- tidy_biden_2021 %>%
  unnest_tokens(word, final_text_2021)
tidy_biden_2021

# Let´s join all the tables:
sentiment_data <- bind_rows(
  tidy_trump_2025, tidy_trump_2017, 
  tidy_biden_2021, tidy_obama_2013)
sentiment_data
# we will not be filtering stopwords since most stopwords (like the, and, is, etc.) are not associated with any emotions in NRC — so they will simply not be counted.

```

#### Joy words and fear words

we are especially interested in the last speech (2025), mainly due to its recent effects. For that reason, we will extract and analyze the most common fear and joy words in his discourses

```{r}
# joy
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

trump2025_joy_words <- sentiment_data %>%
    filter(speech == "Trump_2025") %>%
    inner_join(nrc_joy) %>%
    count(word, sort = TRUE)
trump2025_joy_words

# fear
nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

trump2025_fear_words <- sentiment_data %>%
    filter(speech == "Trump_2025") %>%
    inner_join(nrc_fear) %>%
    count(word, sort = TRUE)

trump2025_fear_words
```

In Trump’s 2025 inaugural speech, the most frequent joy-related words include “good” (20), “money” (18), “beautiful” (17), “love” (13), “luck” (9), “pay” (8), “daughter” (7), “child” (6), “create” (6), and “successful” (6). These words could reflect a strong emphasis on prosperity, traditional family values, and personal success, these words could aim to inspire confidence in economic growth and cultural pride.

Simultaneously, the most common fear-related words: “dangerous” (9), “government” (9), “military” (9), “fight” (8), “illegal” (8), “police” (8), “bad” (7), “inflation” (7), “gang” (6), and “powerful” (5) highlight a narrative of threat and instability, pointing toward concerns about crime, public safety, institutional power, and economic insecurity. The fact that he also mentioned words like “love” and “beautiful” with alarming terms like “dangerous” and “fight” could reveal a strategy centered on contrast: portraying Trump as the defender of a prosperous, idealized America under threat from forces both internal and external.

#### Sentiment changes

Since we have the four most recent presidential talks, we are deeply intrigued by examining how the sentiments change throughout each speech.

Now let´s examine how sentiment changes throughout each speech.

```{r}
evolution_sentiment <- sentiment_data %>%
  #find the sentiment for each word using bing
  inner_join(get_sentiments("bing")) %>%
  #divide each book in chunks of 80 lines
  count(speech, index = linenumber %/% 80, sentiment) %>%
  #we write positive and negative in different columns
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  #we substract positive minus negative to find a net sentiment
  mutate(sentiment = positive - negative)

evolution_sentiment

# We can plot these sentiment scores across the plot trajectory of each speech following the chunks of 80 lines.

ggplot(evolution_sentiment, aes(index, sentiment, fill = speech)) +
  geom_col(show.legend = TRUE) +
  facet_wrap(~speech, ncol = 2, scales = "free_x")

```

The sentiment trajectory across the four inaugural speeches reveals some differences. Trump’s 2025 speech shows the highest variability, with sentiment increasing steadily across chunks and peaking in the middle, suggesting a deliberate emotional buildup aimed at energizing and inspiring his audience toward the end.

In contrast, Trump’s 2017 address maintains a more uniform tone with moderate sentiment, indicating a focus on identifying problems without an emotionally charged resolution.

Obama’s 2013 speech is characterized by steady, more positive sentiment, while Biden’s 2021 speech shows a quick rise in sentiment early on, likely offering hope after acknowledging national challenges, before dipping back to a more neutral tone.

Let's find out how much each word in the speeches contributes to each sentiment. This is: how many times each word appears in the speeches, and which sentiment it is associated to:

```{r}
bing_word_counts <- sentiment_data %>%
  #get sentiment from bing
  inner_join(get_sentiments("bing")) %>%
  #count the number of mentions for each word
  count(word, sentiment, sort = TRUE)

bing_word_counts

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 15) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

This graph highlights the individual words with the greatest contribution to overall positive and negative sentiment across all the inaugural speeches.

On the positive side, the most influential word is "thank", far surpassing others in frequency, suggesting a strong tone of gratitude and acknowledgment (common in inaugural addresses that express appreciation to supporters). Following closely are words like “great,” “like,” “right,” “work,” and “good,” which convey optimism and approval, all values typically emphasized in presidential rhetoric to inspire unity and forward momentum.

On the other hand, the most frequent negative word is “hard,” which can carry a neutral connotation depending on context ("hard work" vs "hard times"), but is still tagged as negative. Other key contributors include “dangerous,” “crisis,” “illegal,” “poverty,” “terrorism,” and “crime,” all of which suggest concern, urgency, or threat. These terms reflect the speeches’ acknowledgment of national and global challenges.

Altogether, the balance of words like “love” and “support” with “crisis” and “threat” can reflect a common dual strategy in political discourse: evoking risk and danger to justify action, while simultaneously promising strength, unity, and improvement.

#### Negative and positive words

Negative and positive words are a way to identify terms that describe something either good, desirable or bad, detrimental. With this idea in mind, we will calculate proportions of negative and positive words and analyze them based on the four speeches we are interested in. Specifically, we are trying to discover:

Which speech has the highest proportion of negative words? Which speech has the highest proportion of positive words?

```{r}
# Negative words ratio
# filter negative words from Bing
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

# make a dataframe (wordcounts) with number of words per speech
wordcounts <- sentiment_data %>%
  group_by(speech) %>%
  summarize(word = n())
wordcounts

#let´s find the number of negative words by speech 
sentiment_data %>%
  #semi_join: returns all words in speech with a match in bingnegative
  semi_join(bingnegative) %>%
  #group by speech to summarize how many negative words
  group_by(speech) %>%
  summarize(negativewords = n()) %>%
  #left_join keeps all words in wordcounts and makes a dataframe
  left_join(wordcounts, by = "speech") %>%
  #create a column in the dataframe with the ratio
  mutate(ratio = negativewords/word) %>%
  #we select the highest ratios
  slice_max(ratio, n = 4) %>% 
  ungroup()

# Positive words ratio
bingpositive <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")

sentiment_data %>%
  semi_join(bingpositive) %>%
  group_by(speech) %>%
  summarize(positivewords = n()) %>%
  left_join(wordcounts, by = "speech") %>%
  mutate(ratio = positivewords/word) %>%
  slice_max(ratio, n = 4)
```

We calculated the number of negative terms (using the Bing lexicon) relative to the total word count for each speech. The results show that Trump’s 2017 speech had the highest negativity ratio, with about 2.66% of its words classified as negative. This is followed by Trump 2025 (2.34%), Biden 2021 (2.21%), and Obama 2013 (1.80%).

This means that Trump's 2017 address stands out as the most negative, proportionally speaking. This aligns with the tone widely observed in that speech, which focused heavily on themes of national decline, threats, and a promise to radically change direction, often using stark or combative language. In contrast, Obama’s speech had the lowest proportion of negative words, reflecting a more optimistic, hopeful, and unifying rhetorical approach.

On the other hand, The next table compares the use of positive words ratios. Trump's 2025 speech stands out with the highest proportion of positive words, at 5.29%, suggesting a more optimistic tone. His 2017 speech also features a relatively high percentage of positive words (4.88%), indicating a similarly positive tone. Obama's 2013 speech shows a slightly lower ratio of 4.72%, reflecting a balanced but still positive rhetoric. In contrast, Biden's 2021 speech has the lowest ratio of positive words, at 3.42%, which could imply a more measured or serious tone.

#### Wordcloud representation

Finally, we will create a visual representation of the data with words as single words, and the importance of each shown with font size. This is called wordcloud representation.

```{r}
sentiment_data %>%
  #we filter stopwords
  anti_join(stop_words) %>%
  #we count words
  count(word) %>%
  #we use the wordcloud function
  with(wordcloud(word, n, max.words = 60))

#set the colors from a brewer palette. 8 is the number of colors used from Dark2 palette.
colors = brewer.pal(10, 'Dark2')

sentiment_data %>%
  anti_join(stop_words) %>%
  count(word) %>%
  #we add colors as an argument
  with(wordcloud(word, n, max.words = 90, colors = colors))

# Compare 2 wordclouds (positive wordcloud with negative wordcloud)
sentiment_data %>%
  #we get sentiments
  inner_join(get_sentiments("bing")) %>%
  #we count word mentions
  count(word, sentiment, sort = TRUE) %>%
  #we establish criteria for size
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  #we paint two wordclouds in one using two different colors
  comparison.cloud(colors = c("blue", "green"),
                   max.words = 100)

# same wordcloud but only restricted to one speech from Trump
sentiment_data %>%
  filter(speech == "Trump_2025") %>%
  #we get sentiments
  inner_join(get_sentiments("bing")) %>%
  #we count word mentions
  count(word, sentiment, sort = TRUE) %>%
  #we establish criteria for size
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  #we paint two wordclouds in one using two different colors
  comparison.cloud(colors = c("blue", "green"),
                   max.words = 100)

```

The wordcloud for Trump’s 2025 speech visually highlights the contrast between positive (green) and negative (blue) language. The largest and most dominant word is “thank,” reflecting a strong positive sentiment (likely due to traditional acknowledgments at the beginning of inaugural speeches). Other prominent positive words include “great,” “like,” “work,” “right,” “good,” and “protect,” which suggest a focus on accomplishment, action, and values like security and fairness.

On the negative side, the most visible terms include “hard,” “illegal,” “dangerous,” “terrorism,” “crisis,” and “poverty.” These reflect a discourse centered around threat, disorder, and societal challenges. The size of these words indicates they were repeated often, contributing to the speech's emotionally charged tone.

### Term frequency

The next tool to be used in the analysis is *term frequency* (TF), which measures how often a word appears in a document. However, to better capture the relevance of words across multiple documents, we will also use *inverse document frequency* (IDF), which reduces the weight of terms that are common across many texts. By combining both, we obtain *TF-IDF*, a metric that highlights words that are not only frequent in a given document but also distinctive compared to the rest of the corpus. This helps us identify terms that are truly characteristic of each speech.

#### Most frequently used words

We start by analysing the most often used words. Firstly we will obtain the most frequent words in the whole speeches and then the total number of words per president.

```{r}
# Transform the texts into one character
obama2013 <- paste(final_text_2013, collapse = " ")
trump2017 <- paste(final_text_2017, collapse = " ")
biden2021 <- paste(final_text_2021, collapse = " ")
trump2025 <- paste(final_text_2025, collapse = " ")

# We create the tibble
speech <- tibble(
  year = as.factor(c(2013, 2017, 2021, 2025)),
  text = c(obama2013, trump2017, biden2021, trump2025))

# Tokenization
speech_total <- speech %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)
speech_total
```

Now we have a dataframe with the number of times each word appears in all the texts available. The next step is to obtain the number of times the words appear in each speech.

```{r}
speech_words <- speech %>%
  unnest_tokens(word, text) %>%
  count(year, word, sort = TRUE)
```

In order to be able to proceed further with the analysis of term frequency, we sum up the total number of words in each speech. Then we have to add these numbers to the dataframe created to use them in the following steps.

In this case we have not filtered out the stop words as they are necessary to obtain the total number of words in each of the texts.

```{r}
total_words <- speech_words %>% 
  group_by(year) %>% 
  summarize(total = sum(n)) # Number of words per speech

speech_words <- left_join(speech_words, total_words) 
# We keep all the variables in the df
speech_words
```

With this information we can now obtain the term frequency distribution for each of the speeches we have selected. This data is obtained by dividing the number of times a word appears in a text divided by the total number of terms (words) in that text.

```{r}
speech_words <- speech_words %>%
  # We add a column for term_frequency
  mutate(term_frequency = n/total)

speech_words
```

Let's make a plot to see how words are distributed in each discourse.

```{r}
ggplot(speech_words, aes(term_frequency, fill = year)) +
  geom_histogram(show.legend = TRUE) +
  facet_wrap(~ year, ncol = 2, scales = "free_y") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal()
```

As can be seen, there is a similar distribution for all years: most words have very low frequencies and a few words have very high frequencies.

Let's look at the distribution of less frequent words.

```{r}
ggplot(speech_words, aes(term_frequency, fill = year)) +
  geom_histogram(show.legend = TRUE) +
  xlim(NA, 0.001) +
  facet_wrap(~ year, ncol = 2, scales = "free_y") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal()
```

The distribution seems to remain very similar for all speeches, with the number of words decreasing with increasing frequency.

Once we have been able to work with the term frequency tool, we can move forward using a number of other measures that can help us to learn more about these texts.

#### Zipf's Law

The next tool that we will use is the *Zipf’s Law*. This law states that a word’s frequency is inversely proportional to its rank in a frequency list: the most common words appear most often, while rare words have higher ranks and appear less frequently.

Let's create a new dataframe for our data with a new column that ranks the words in descending order by their term frequency.

```{r}
freq_by_rank <- speech_words %>% 
  group_by(year) %>% 
  mutate(rank = row_number()) %>%
  ungroup()

freq_by_rank
```

Let's visualise in a line graph the inversely proportional relationship between TF and the rank of the words we are working with.

```{r}
freq_by_rank %>% 
  ggplot(aes(rank, term_frequency, color = year)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() +
  scale_color_brewer(palette = "Set2") +
  theme_minimal()
```

The logarithmic scale shows that there is a constant negative slope. However, as it can be seen, there are some slight differences in the distributions of the four discourses. To understand these variations, we need to divide the graph into three parts and analyse them separately.

It seems that the central part is the most stable, as it is where all the distributions coincide the most. In order to better analyse the texts, we will create a linear regression model using this central section and then display it in the graph. In this way, we use this section as a normal use of the language and we can check how much the other sections differ.

```{r}
rank_subset <- freq_by_rank %>% # We store the section in a new variable
  filter(rank < 400,
         rank > 10) # Ranks between 400 and 10

# We use the linear model function
lm(log10(term_frequency) ~ log10(rank), data = rank_subset)

# We visualize it on the graph by adding a reference line
freq_by_rank %>% 
  ggplot(aes(rank, term_frequency, color = year)) + 
  geom_abline(intercept = -0.62, slope = -1.1, 
              color = "gray30", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10() +
  scale_color_brewer(palette = "Set2") +
  theme_minimal()
  
```

The most important thing to note in this graph is that less frequent words appear more frequently in these discourses. It may imply that these speeches are either very issue-specific or use more artificial language as a political tool. We will take this into account for the next steps.

#### Obtaining the TF-IDF

The next step is to obtain the IDF and then combine it with the term frequency measure. To do this, we just have to apply the *bind_tf_idf* from the *tidytext* package to our dataset.

```{r}
speech_tf_idf <- speech_words %>%
  bind_tf_idf(word, year, n) %>% # We obtain three new columns
  select(-term_frequency) # We can drop the TF that we had before 

speech_tf_idf
```

The first words that appear have a TF-IDF close to zero because they are used so often. It can be observed that these are the same words for the four discourses analysed (*for*, *and*, *the*, *of*...).

Let's sort them in descending order according to the TF-IDF measure to see the strangest words in the whole collection.

```{r}
sorted_tf_idf <- speech_tf_idf %>%
  select(-total) %>% # This column is not necessary anymore
  arrange(desc(tf_idf))

sorted_tf_idf
```

What is most striking is the number of times Trump uses the adverb *very* in 2025. On the other hand, as the date is very close to the Covid-19 pandemic, it is logical to see that Biden in 2021 is by far the one who uses the word *pandemic* the most.

In order to better observe the differences between the four speeches, we will group the words by year and thus obtain the 10 most special words of each of them in comparison with the rest.

```{r}
# Modify the titles of each graph
president <- c(
  "2013" = "Obama (2013)",
  "2017" = "Trump (2017)",
  "2021" = "Biden (2021)",
  "2025" = "Trump (2025)")

# We create the plot
plot_speech <- speech_tf_idf %>% 
  group_by(year) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = year)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~ year, ncol = 2, 
             scales = "free",
             # President and year for the titles
             labeller = labeller(year = president)) +
  # We modify the limits to compare better the differences
  scale_x_continuous(limits = c(0, 0.0025)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))

plot_speech
```

As can be seen in the two Trump bar plots, the word very appears at the top without adding too much value to the interpretation. It is better to put it as a stopword and then running the plot again. We can also add the nouns mentioned in the first Trump speech.

```{r}
trump_stop <- tibble(word = c("very", "susan", "oliver", "megan´s", 
                              "jenna", "jamiel", "250th"))

speech_tf_idf <- speech_tf_idf |> 
  anti_join(trump_stop, by = "word")

plot_speech <- speech_tf_idf %>% 
  group_by(year) %>% 
  slice_max(tf_idf, n = 10, with_ties = FALSE) %>% 
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = year)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~ year, ncol = 2, 
             scales = "free",
             # President and year for the titles
             labeller = labeller(year = president)) +
  # We modify the limits to compare better the differences
  scale_x_continuous(limits = c(0, 0.0025)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))

plot_speech
```

The conclusions are similar to those previously discussed. By analysing each speech separately, it is striking that Obama in 2013 uses initiative-laden language using words such as let's or encourage. On the other hand, Biden's speech in 2021 is loaded with mentions of covid and vaccines, although the importance of guns is also very relevant.

On the other hand, analysing Trump's speeches separately also allows for some very relevant conclusions. First of all, although we tried to filter out the names included in the speech of 2017, it seems that it is a long list because we still get more names. Another thing that stands out in the 2017 speech is the multiple mentions of [*Obamacare*](https://www.usa.gov/es/salud-mercado-seguros-medicos-aca) and health insurance. These data denote a clear criticism of Barack Obama's previous government, which is complemented by personal stories using the personal names adressed before (which also appear in the most relevant words of this speech). In 2025 it highlights the importance he attaches in his speech to Ukraine and to the tariff measures he had previously announced. The rest of the words do not provide us with much more information beyond being words that we need to understand in context.

With the sentiment analysis and term frequency tools, we were able to get an idea of the content and characteristics of the discourses. However, it is necessary to supplement these results with topic modelling in order to draw more developed conclusions.

### Topic modeling

We will now start with the last technique, Topic Modeling. This is a common technique in natural language processing used to discover hidden themes in large collections of text. Our goal with topic modeling is to uncover the main topics discussed in the inauguration speeches and understand how they vary between presidents. First, we will preprocess the text to build a document-term matrix, then we will apply Latent Dirichlet Allocation (LDA) to extract a set of dominant topics. We will analyze the top words associated with each topic and we will conclude with a visualization of the topic distribution across different speeches.

We will start by analyzing Trump's speeches, for being able to compare them to the other president's speeches later on.

#### Trump 2017 speech

##### Convert the text into a data frame

In order to be able to analyze the text, we need to convert it into a structured data frame, where each line of speech has an identifier (line). This facilitates the following cleaning and analysis operations. As a result, we obtain a tibble with two columns: line (line number) and text (textual content).

```{r}
df_trump <- tibble(line = 1:length(final_text_2017), text = final_text_2017)
```

After having our tibble, we need to tokenize and clean the text. Tokenization and text cleaning are essential preprocessing steps in any text mining workflow. Tokenization involves breaking down the text into individual words, or tokens, which allows us to analyze the text at the word level. After tokenizing, we perform additional cleaning to remove elements that do not contribute meaningful information to the analysis. This includes removing common stopwords (such as "the", "and", "we"), which appear frequently but carry little semantic weight, and filtering out numerical values, which can introduce noise into the topic model. By focusing only on the most relevant words, we enhance the clarity and coherence of the topics that the LDA algorithm will later identify.

```{r}
df_words <- df_trump %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%  # filter stopwords
  filter(!str_detect(word, "^[0-9]+$"))  # take out numbers 
```

As a result, we obtain a dataframe with columns line and word, which contains only significative and useful words.

Now that we have our data tokenized and cleaned, we need to convert it into a DTM format matrix in which rows are lines of speech, columns are words and values are the number of occurrences of each word. This matrix is the input needed by the LDA model. The result will be a DTM matrix with word counts per line.

```{r}
df_dtm <- df_words %>%
  count(line, word) %>%
  cast_dtm(document = line, term = word, value = n)
```

##### Apply LDA

Once we have our cleaned and tokenized text represented as a DTM, we can use LDA to uncover the hidden thematic structure within the text. LDA is a probabilistic generative model that assumes each document (in our case, each line of the speech) is a mixture of several topics, and each topic is characterized by a distribution over words. When we apply LDA, we specify the number of topics (k) that we want the algorithm to identify. Internally, the model iteratively estimates two probability distributions: the distribution of topics across documents (gamma), and the distribution of words across topics (beta). These distributions are computed using an optimization process that assigns words to topics in a way that maximizes the likelihood of the observed data. In this step, we also fix a random seed to ensure reproducibility. The main output of this step is the trained LDA model object, which contains all the information needed to interpret and visualize the topics. Specifically, we will later extract the most representative words for each topic (using beta) and the topic proportions for each document (using gamma), which will allow us to understand how different parts of the speech relate to the identified topics.

```{r}
lda_model <- LDA(df_dtm, k = 4, control = list(seed = 1234)) # Supposing we want to find 4 themes
```

Now that we have our LDA performed, we will use "matrix = "beta"" to extract the probability of a word given a theme. We will choose the 10 most representative words for each theme and we will visualize the results in a graph.

```{r}
# Extract the most representative terms by topic
topics_terms <- tidy(lda_model, matrix = "beta")

# 10 most relevant words by theme 
top_terms <- topics_terms %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualization
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms by theme",
       x = NULL, y = expression(beta))  +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

In the visualization above each facet corresponds to a topic, and the length of the bars reflects the probability (β) that each word belongs to that topic. For example, in Topic 1, terms like great, america, country, and people are prominent, suggesting that this topic may relate to patriotic rhetoric and national pride. Topic 2 includes words such as us, must, american, states, and support, indicating a theme around collective action, duty, and perhaps military or international alliances. In Topic 3, terms like health, infrastructure, insurance, and government point toward domestic policy issues such as healthcare and public services. Topic 4 contains words such as immigration, system, believe, and nations, which likely refer to foreign policy and immigration reform. However, some overlapping terms like american and america appear across multiple topics, which may reduce topic distinctiveness. This overlap can be common in political speeches where key thematic words are repeated frequently.

That is why, we will improve the quality of the topics identified. In this case, the graph shows some very repetitive and not very distinctive words that appear in several themes and do not help to differentiate well between them. We will go back to the cleanup step and manually remove frequent words that were not automatically removed, such as “america”, “american”, “must”, “one”, etc.; we will re-generate the DTM and re-apply the LDA model and test with different values of k to see if the topics are more coherent.

```{r}
# Personalized stopword list
custom_stopwords <- c("america", "american", "must", "just", "one", "new", "us", "like", "also", "now", "get", "even")

# Tokenization and cleaning
df_words_refined <- df_trump %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%
  filter(!word %in% custom_stopwords) %>%
  filter(!str_detect(word, "^[0-9]+$"))

# Re-create DTM
df_dtm_refined <- df_words_refined %>%
  count(line, word) %>%
  cast_dtm(document = line, term = word, value = n)

# Apply LDA
lda_model_refined <- LDA(df_dtm_refined, k = 4, control = list(seed = 1234))

# Visualize new themes
topics_terms_refined <- tidy(lda_model_refined, matrix = "beta")

top_terms_refined <- topics_terms_refined %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_refined %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms by topic (refined)",
       x = NULL, y = expression(beta)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

The updated visualization reveals more coherent and meaningful themes compared to the initial version. After refining the preprocessing step and removing generic, high-frequency words like “america” and “american”, the model now produces topics with more distinct and interpretable vocabularies. For instance, Topic 1 includes terms such as great, country, health, administration, and citizens, suggesting a focus on national pride, governance, and public services. Topic 2, with words like every, world, year, united, and history, appears to address broader, possibly international or historical themes and global cooperation. Topic 3 is strongly characterized by the presence of words like system, immigration, create, and access, pointing towards themes of immigration reform, policy systems, and support initiatives. Finally, Topic 4 contains terms like terrorism, trade, respect, and allies, clearly reflecting concerns about foreign policy, international relations, and national security. Overall, this version of the model shows better topic separation and thematic clarity.

We will also try different values of k, for example 3 or 5, to see if the topics are more consistent.

```{r}
# Change value of k and make the model again
k_values <- c(3, 5)

lda_models <- list()

for (k in k_values) {
  model <- LDA(df_dtm_refined, k = k, control = list(seed = 1234))
  lda_models[[as.character(k)]] <- model
}

# Try with k = 5
topics_terms_k5 <- tidy(lda_models[["5"]], matrix = "beta")

top_terms_k5 <- topics_terms_k5 %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_k5 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms by topic (k = 5)",
       x = NULL, y = expression(beta)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))

# Try with k = 3
topics_terms_k3 <- tidy(lda_models[["3"]], matrix = "beta")

top_terms_k3 <- topics_terms_k3 %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_k3 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms by topic (k = 3)",
       x = NULL, y = expression(beta)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

When comparing the LDA models with k = 3 and k = 5, we observe noticeable differences in the specificity and interpretability of the topics. The model with k = 3 generates broader, more general topics. For instance, Topic 1 and Topic 2 include terms like country, people, world, and united, which overlap significantly and may reduce the semantic distinction between topics. Topic 3 in this model clusters words related to policy and immigration (system, immigration, help, create), but the grouping still feels somewhat mixed and less thematically tight. This model captures the overall themes of the speech, but the topics are somewhat diluted and less actionable for deeper insight.

In contrast, the k = 5 model presents more clearly defined topics, each centered around a specific aspect of the speech. For example, Topic 1 contains personal and ceremonial terms like tonight, thank, ryan, and great, suggesting emotional or introductory language. Topic 2 reflects economic and administrative themes (companies, tax, jobs, borders), while Topic 3 seems to highlight American citizens and support (help, choice, access). Topic 4 is strongly focused on immigration and security (terrorism, immigration, system, respect), and Topic 5 touches on diplomatic and visionary rhetoric (allies, believe, future, free). This greater topic resolution offers clearer insights into the structure and rhetoric of the speech, making k = 5 a more effective choice for interpretability in this context.

So, we decide to use the LDA model with k = 5 because it offers a clearer and more interpretable separation between topics.

```{r}
lda_final <- lda_models[["5"]]
```

We will extract the top words for each topic again.

```{r}
# Extract top words for each topic
topics_final <- tidy(lda_final, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  arrange(topic, -beta)

# View table of top terms
topics_final %>%
  select(topic, term, beta) %>%
  knitr::kable() %>%
  kableExtra::kable_styling(full_width = FALSE)
```

So, although the interpretation of topics based on top terms has already been done, we can conclude that the main topics are: 1. Ceremonial and Governance 2. Economy and Policy 3. Support and Personal Appeals 4. Security and Immigration 5. Vision and Diplomacy

##### Assigning themes to each line of the speech

We will proceed by using the gamma matrix to identify which topic is most dominant in each line of the speech.

```{r}
# Topic probabilities for each document
doc_topics <- tidy(lda_final, matrix = "gamma")

# Get dominant topic per line
dominant_topic <- doc_topics %>%
  group_by(document) %>%
  slice_max(gamma, n = 1)

# Merge with original text
df_trump_topics <- df_trump %>%
  mutate(document = as.character(line)) %>%
  left_join(dominant_topic, by = "document")

```

We will create 3 different ways to visualize the results:

First we will view representative examples by theme. This will help understand qualitatively what type of content falls under each theme, also helping confirm that the topic assignments match their intended meaning.

```{r}
# 3 representative lines by each topic
df_trump_topics %>%
  group_by(topic) %>%
  slice_sample(n = 3) %>%
  select(topic, text) %>%
  arrange(topic)

```

We will also analyze the frequency of each topic in the speech. This will tell us which topics dominate the speech. For example, if Topic 2 (Economy) has the most lines, that might suggest the economy was a central theme in this address.

```{r}
# Count how many lines are from each topic
df_trump_topics %>%
  count(topic) %>%
  ggplot(aes(x = factor(topic), y = n, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Number of lines per topic", x = "Topic", y = "Line count") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(axis.title = element_text(size = 10, face = "bold"))
```

Finally, we will watch the evolution of topics along the speech. This will help us analyze structure and flow.

```{r}
# Change of topics all along the speech
df_trump_topics %>%
  mutate(line = as.numeric(document)) %>%
  ggplot(aes(x = line, y = topic, color = factor(topic))) +
  geom_point() +
  labs(title = "Topic flow throughout the speech", x = "Line number", y = "Topic") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(axis.title = element_text(size = 10, face = "bold"))
```

After assigning the most likely topic to each line of President Trump’s 2017 speech, we examined the results and these are the results:

The frequency analysis reveals that Topic 3 dominates the speech, followed by Topics 2 and 5, while Topic 4 is the least frequent. This suggests that the speech is heavily focused on citizen-related appeals, social policy, and support systems (Topic 3), along with economic and governmental issues (Topic 2), and visionary or diplomatic language (Topic 5). In contrast, security and immigration (Topic 4) occupies a smaller portion, indicating it was treated as a specific section rather than a recurring theme.

The topic progression plot shows how these themes unfold across the speech. Notably, the speech does not follow a strict thematic block structure. Instead, there is a scattered distribution, with topics appearing throughout in varying proportions. For example, Topic 1 (ceremonial and general statements) appears early and then reappears toward the end, which is consistent with how speeches often open and close with broad, unifying messages. Topic 3 is present across the entire speech, indicating a continuous effort to relate to individual citizens and emotional narratives. Topic 4, though less frequent, appears in mid-speech clusters, corresponding to sections on immigration and national security.

The sample lines per topic reinforce the thematic interpretations. Lines categorized under Topic 3 include phrases about helping Americans, improving access to healthcare, and personal stories; confirming it as a socially oriented, emotionally resonant topic. Topic 4 includes discussions about terrorism, borders, and immigration, aligning well with its security-driven vocabulary. Topics 1, 2, and 5 also maintain internal consistency in their sample texts, validating the model's accuracy.

Overall, thematic analysis of Trump’s 2017 State of the Union reveals a speech organized around recurring priorities, delivered in a clear and structured way. The five topics identified through LDA include themes such as ceremonial praise and American greatness, policy and legislative direction, support for American citizens, immigration and security concerns, and national vision in foreign affairs. The topic flow across the speech shows that Trump frequently transitions between these themes, creating a dynamic rhythm that blends emotion, persuasion, and political messaging. The distribution of topics reflects a speech focused on rallying national pride while addressing both domestic and global priorities. So, the thematic structure effectively supports the tone and political messaging of the address.

All in all, the topic modelling helped break down the speech into clear themes and showed how they’re spread across the text. It also made it easier to understand how Trump uses different topics to connect with the audience and deliver his message more effectively.

We will now proceed to do the same thing with the other 3 speeches we want to analyze, but we will make the explanation shorter, as we have already discussed everything in this first analysis. We will just replicate what has been done, in order to be able to compare the 4 speeches at the end.

#### Trump 2025 speech

We first need to clean the text and make it in a suitable format for practicing topic modeling.

We will now convert it to a dataframe and apply tokenization.

```{r}
# Convert into a dataframe
df_2025 <- tibble(line = 1:length(final_text_2025), text = final_text_2025)

# Tokenization and cleaning
df_words_2025 <- df_2025 %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%               
  filter(!str_detect(word, "^[0-9]+$"))        
```

Now that we have our data tokenized and cleaned, we need to convert it into a DTM.

```{r}
df_dtm_2025 <- df_words_2025 %>%
  count(line, word) %>%
  cast_dtm(document = line, term = word, value = n)
```

Once we have our cleaned and tokenized text represented as a DTM, we can use LDA to uncover the hidden thematic structure within the text. We will use K = 5 in order to be able to compare all the speeches at the end.

```{r}
lda_2025 <- LDA(df_dtm_2025, k = 5, control = list(seed = 1234))
```

Now that we have our LDA performed, we will use "matrix = "beta"" to extract the probability of a word given a theme. We will choose the 10 most representative words for each theme and we will visualize the results in a graph.

```{r}
# Extract the most representative terms by topic
topics_2025 <- tidy(lda_2025, matrix = "beta")

# 10 most relevant words by theme 
top_terms_2025 <- topics_2025 %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualization
top_terms_2025 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms per topic (Trump 2025 Speech)",
       x = NULL, y = expression(beta)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

In this case, several high-frequency words like “applause”, “it's”, and “going” appeared in multiple topics without adding real semantic value. Removing these can help the model focus on more meaningful content and improve the clarity and coherence of the topics, so, we need to create a custom list of stopwords and do the whole process again.

```{r}
# List of customized stopwords
custom_stopwords <- c("applause", "it’s", "just", "also", "thank", "going", "we’re",
                      "new", "get", "great", "now", "back", "first", "much", "want",
                      "never", "he’s", "didn’t", "don’t", "they’re", "i’m") 

# Convert into dataframe
df_2025_clean <- tibble(line = 1:length(final_text_2025), text = final_text_2025)

# Tokenization + cleaning
df_words_2025_clean <- df_2025_clean %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%                         
  filter(!word %in% custom_stopwords) %>%                
  filter(!str_detect(word, "^[0-9]+$"))                  
```

We will now convert it into DTM and apply LDA again.

```{r}
# Convert into DTM
df_dtm_2025_clean <- df_words_2025_clean %>%
  count(line, word) %>%
  cast_dtm(document = line, term = word, value = n)

# Apply LDA
lda_2025_clean <- LDA(df_dtm_2025_clean, k = 5, control = list(seed = 1234))
```

And we will recreate the visualization in order to see if the information obtained is more useful and informative.

```{r}
# Extract the most representative terms by topic
topics_2025_clean <- tidy(lda_2025_clean, matrix = "beta")

# 10 most relevant words by theme
top_terms_2025_clean <- topics_2025_clean %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualization
top_terms_2025_clean %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms per topic (Trump 2025 – Improved Cleaning)",
       x = NULL, y = expression(beta)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

The final version of Trump’s 2025 speech offers a well-structured and thematically clear breakdown. Topic 1 includes words like “years”, “tonight”, “members”, “history”, and “days”, which seem to relate to ceremonial references, past accomplishments, or commemorative remarks (possibly the opening of the speech or reflective segments). Topic 2 centers around “good”, “beautiful”, “job”, “luck”, and “hard”, indicating an optimistic tone focused on personal achievements, character traits, or positive evaluations of people and efforts. This likely reflects moments of praise or uplifting rhetoric. Topic 3 is characterized by “million”, “money”, “world”, “country”, “tax”, and “united states”, suggesting a strong emphasis on economic issues, national finances, and global positioning (themes common in economic and geopolitical sections of the speech). Topic 4 includes terms like “us”, “fight”, “make”, “money”, and “congress”, which indicate a call to action, national strength, and perhaps direct political challenges or legislative goals. Finally, Topic 5 stands out for including more unique and specific terms like “panama”, “roberto”, “canal”, and “administration”, hinting at a reference to foreign policy or a historical anecdote about international relations and infrastructure (likely a specific story or example included for rhetorical effect).

We will proceed by using the gamma matrix to identify which topic is most dominant in each line of the speech.

```{r}
# Gamma matrix
doc_topics_2025 <- tidy(lda_2025_clean, matrix = "gamma")

# Select dominant topic by line
dominant_topic_2025 <- doc_topics_2025 %>%
  group_by(document) %>%
  slice_max(gamma, n = 1)

# Join with discourse lines
df_trump_topics_2025 <- df_2025 %>%
  mutate(document = as.character(line)) %>%
  left_join(dominant_topic_2025, by = "document")
```

And now we can visualize the results.

```{r}
# Representative examples by topic
df_trump_topics_2025 %>%
  group_by(topic) %>%
  slice_sample(n = 3) %>%  # 3 examples by topic
  select(topic, text) %>%
  arrange(topic)

# Number of lines per topic
df_trump_topics_2025 %>%
  count(topic) %>%
  ggplot(aes(x = factor(topic), y = n, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Number of lines per topic", x = "Topic", y = "Line count") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(axis.title = element_text(size = 10, face = "bold"))

# Topic evolution throughout the discourse
df_trump_topics_2025 %>%
  mutate(line = as.numeric(document)) %>%
  ggplot(aes(x = line, y = topic, color = factor(topic))) +
  geom_point() +
  labs(title = "Topic flow throughout the speech", x = "Line number", y = "Topic") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(axis.title = element_text(size = 10, face = "bold"))
```

The topic assignment reveals a well-distributed thematic structure throughout the speech. From the bar chart, we can observe that Topics 2 and 3 are the most dominant, each appearing in over 70 lines. These likely represent the speech’s core themes—potentially centered on economy, job creation, or patriotic sentiments—depending on how they were previously interpreted. Topic 1 also has strong representation (around 55 lines), possibly related to commemorative or ceremonial aspects of the address. Topics 4 and 5 show slightly fewer occurrences but are still meaningfully present. Topic 4 could correspond to policy initiatives or calls to action, while Topic 5 may deal with foreign affairs or historical references, based on previous top term analysis. Interestingly, a few lines remain unclassified (NA). These are likely short or vague segments (“Thank you very much”), which don’t contain enough information for the LDA model to confidently assign them to a topic.

The flow plot gives insight into the speech’s structure: We see that topics are interleaved throughout the text, rather than being grouped in large thematic blocks. This indicates that Trump frequently switches between themes, perhaps to maintain engagement or touch on multiple talking points in close succession. Topic 2 and Topic 3 appear consistently throughout, showing they are central pillars of the speech, while Topics 4 and 5 appear more sporadically.

The topic modeling results for Trump’s 2025 address point to a speech with a strong focus on values, security, and public reassurance. The five topics extracted reflect recurring themes like patriotic rhetoric and government action, economic concerns, emotional appeals and recognition of individuals, national security and border control, and America’s place in the world. Thematic flow is consistent throughout the speech, with repeated references to key ideas like prosperity, protection, and leadership. This back-and-forth between emotional storytelling and assertive policy language keeps the speech engaging while reinforcing Trump’s narrative style. Overall, the topic distribution captures the dual nature of the address—both performative and policy-driven.

With this being said and having analyzed Trump's speeches, we will now move on to Obama's speech.

#### Obama 2013 speech

We first need to clean the text and make it in a suitable format for practicing topic modeling.

We will now convert it to a dataframe and apply tokenization.

```{r}
# Convert it to a dataframe
df_obama_2013 <- tibble(line = 1:length(final_text_2013), text = final_text_2013)

# Tokenization + cleaning
df_words_obama <- df_obama_2013 %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%
  filter(!str_detect(word, "^[0-9]+$"))  
```

Now that we have our data tokenized and cleaned, we need to convert it into a DTM.

```{r}
df_dtm_obama <- df_words_obama %>%
  count(line, word) %>%
  cast_dtm(document = line, term = word, value = n)
```

Once we have our cleaned and tokenized text represented as a DTM, we can use LDA to uncover the hidden thematic structure within the text. We will use K = 5 in order to be able to compare all the speeches at the end.

```{r}
lda_obama <- LDA(df_dtm_obama, k = 5, control = list(seed = 1234))
```

Now that we have our LDA performed, we will use "matrix = "beta"" to extract the probability of a word given a theme. We will choose the 10 most representative words for each theme and we will visualize the results in a graph.

```{r}
# Extract the most representative terms by topic
topics_obama <- tidy(lda_obama, matrix = "beta")

# 10 most relevant words by theme 
top_terms_obama <- topics_obama %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualization
top_terms_obama %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms per topic (Obama 2013)",
       x = NULL, y = expression(beta)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

The results for President Obama’s speech appear thematically coherent and relatively well-balanced across the five topics. Topic 1 is dominated by terms like “energy”, “years”, “gas”, “power”, and “home”, suggesting a focus on energy policy, sustainability, and infrastructure. Topic 2, with terms such as “can”, “tonight”, “congress”, “jobs”, and “act”, reflects the persuasive and directive tone of the speech, particularly calls to legislative action or government cooperation. Topic 3 is clearly focused on employment and the economy, with words like “jobs”, “high”, “tax”, “education”, and “job”, making this theme highly relevant to middle-class concerns and recovery plans. Topic 4 contains terms like “time”, “cuts”, “work”, “need”, and “americans”, which likely reflects urgency in addressing fiscal responsibility or policy reform—possibly linked to the budget and deficit debates at the time. Topic 5 stands out with “vote”, “families”, “deserve”, “right”, and “must”, suggesting a civic-oriented or value-driven message, possibly touching on themes like voting rights, equality, and social justice.

We will proceed by using the gamma matrix to identify which topic is most dominant in each line of the speech.

```{r}
# Gamma matrix
obama_gamma <- tidy(lda_obama, matrix = "gamma")

# Select dominant topic for each line
dominant_topic_obama <- obama_gamma %>%
  group_by(document) %>%
  slice_max(gamma, n = 1)

# Merge with original text 
df_obama_topics <- df_obama_2013 %>%
  mutate(document = as.character(line)) %>%
  left_join(dominant_topic_obama, by = "document")
```

And we visualize the results.

```{r}
# Representative examples by topic
df_obama_topics %>%
  group_by(topic) %>%
  slice_sample(n = 3) %>%  
  select(topic, text) %>%
  arrange(topic)

# Number of lines per topic
df_obama_topics %>%
  count(topic) %>%
  ggplot(aes(x = factor(topic), y = n, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Number of lines per topic", x = "Topic", y = "Line count") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(axis.title = element_text(size = 10, face = "bold"))

#Topic flow throughout the speech
df_obama_topics %>%
  mutate(line = as.numeric(document)) %>%
  ggplot(aes(x = line, y = topic, color = factor(topic))) +
  geom_point() +
  labs(title = "Topic flow throughout the speech", x = "Line number", y = "Topic") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(axis.title = element_text(size = 10, face = "bold"))
```

The distribution of topics across the speech is fairly balanced, as shown in the bar chart. All five topics have a similar number of lines, suggesting that Obama structured his address to cover multiple key areas fairly evenly. From the sample lines per topic: Topic 1 includes lines about legal immigration, hardworking Americans, and national remembrance. This points to citizenship, values, and national unity. Topic 2 seems more oriented toward policy and legislative action, featuring words like “congress”, “act”, and “jobs”. Topic 3 has strong economic content: lines on job creation, infrastructure, manufacturing, and the Affordable Care Act. This clearly represents the economy and domestic programs. Topic 4 lines include budget-focused language such as “cuts”, “work”, “need”, and “time”, highlighting urgency and economic reform. Topic 5 involves more emotive and civic language (“vote”, “families”, “deserve”, “unfinished task”), signaling a call to civic responsibility and social justice. The flow plot shows that Obama alternates between topics rather than presenting them in large uninterrupted blocks. This kind of thematic interweaving suggests a rhetorical strategy that ties together different priorities—values, legislation, economy—within broader arguments.

Overall, thematic analysis of Obama’s 2013 State of the Union shows a well-structured and balanced speech, covering a range of key issues. The five topics identified through LDA span themes like immigration and national values, economic recovery and job creation, legislative action, fiscal reform, and civic responsibility. The flow of topics throughout the speech highlights Obama’s rhetorical strategy of revisiting core themes rather than isolating them into separate sections. This helps maintain coherence and momentum, while also reinforcing the connection between policy goals and American values. So, the topic distribution and progression align well with the broader tone and goals of the address.

We will move on to the last discourse, Biden's 2021 speech.

#### Biden 2021 speech

We first need to clean the text and make it in a suitable format for practicing topic modeling.

We will now convert it to a dataframe and apply tokenization.

```{r}
# Convert it to a dataframe
df_biden_2021 <- tibble(line = 1:length(final_text_2021), text = final_text_2021)

# Tokenization + cleaning
df_words_biden <- df_biden_2021 %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%
  filter(!str_detect(word, "^[0-9]+$")) 
```

Now that we have our data tokenized and cleaned, we need to convert it into a DTM.

```{r}
df_dtm_biden <- df_words_biden %>%
  count(line, word) %>%
  cast_dtm(document = line, term = word, value = n)
```

Once we have our cleaned and tokenized text represented as a DTM, we can use LDA to uncover the hidden thematic structure within the text. We will use K = 5 in order to be able to compare all the speeches at the end.

```{r}
lda_biden <- LDA(df_dtm_biden, k = 5, control = list(seed = 1234))
```

Now that we have our LDA performed, we will use "matrix = "beta"" to extract the probability of a word given a theme. We will choose the 10 most representative words for each theme and we will visualize the results in a graph.

```{r}
# Extract the most representative terms by topic
topics_biden <- tidy(lda_biden, matrix = "beta")

# 10 most relevant words by theme 
top_terms_biden <- topics_biden %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualization
top_terms_biden %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms per topic (Biden 2021)",
       x = NULL, y = expression(beta)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

In this case, several high-frequency words like “we're”, “it's”, and “one” appeared in multiple topics without adding real semantic value. Removing these can help the model focus on more meaningful content and improve the clarity and coherence of the topics, so, we need to create a custom list of stopwords and do the whole process again.

```{r}
biden_stopwords <- c("it’s", "we’re", "i’ve", "that’s", "let’s", "one", "just", "can", "said", "know", "ever", "get", "now", "also", "percent", "act", "america", "american")

# Convert into a tibble
df_biden_2021 <- tibble(line = 1:length(final_text_2021), text = final_text_2021)

# Tokenize +cleaning
df_words_biden <- df_biden_2021 %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% biden_stopwords) %>%                     
  anti_join(get_stopwords()) %>%                          
  filter(!str_detect(word, "^[0-9]+$"))                    

# Create DTM
dtm_biden <- df_words_biden %>%
  count(line, word) %>%
  cast_dtm(document = line, term = word, value = n)

# LDA with  k = 5
lda_biden <- LDA(dtm_biden, k = 5, control = list(seed = 1234))

# Top terms per topic
top_terms_biden <- tidy(lda_biden, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Visualization
top_terms_biden %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms per topic (Biden 2021 – Improved Cleaning)",
       x = NULL, y = expression(beta)) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

The refined output from Biden's 2021 speech shows a much clearer thematic structure. Topic 1 features terms like century, years, world, and tax, suggesting a focus on long-term planning and economic positioning of the U.S. in the 21st century. Topic 2, with words such as jobs, power, plan, change, and investments, clearly reflects economic recovery and industrial policy, a recurring theme in Biden's economic messaging. Topic 3 blends terms like families, care, jobs, and need, pointing toward social policy, healthcare, and family support. Topic 4, containing words like pass, congress, right, and pandemic, indicates legislative appeals and crisis response, likely tied to COVID-19 and civil rights discussions. Lastly, Topic 5 uses keywords such as people, democracy, gun, opportunity, and justice, all of which suggest a strong emphasis on American identity, civic values, and calls for unity. Overall, the model has successfully isolated key policy pillars of Biden’s speech—economic strategy, healthcare, legislative action, and democratic ideals—with clear differentiation between topics.

We will proceed by using the gamma matrix to identify which topic is most dominant in each line of the speech.

```{r}
# Gamma matrix
biden_topics <- tidy(lda_biden, matrix = "gamma")

# Select dominant topic for each line
biden_top_topic <- biden_topics %>%
  group_by(document) %>%
  slice_max(gamma) %>%
  ungroup()

# Merge with original text 
df_biden_assigned <- df_biden_2021 %>%
  mutate(document = as.character(line)) %>%
  inner_join(biden_top_topic, by = "document") %>%
  select(line, text, topic)

```

And we visualize the results.

```{r}
# Representative examples by topic
df_biden_assigned %>%
  group_by(topic) %>%
  slice_head(n = 3) %>%  
  select(topic, text) %>%
  arrange(topic)

# Number of lines per topic
df_biden_assigned %>%
  count(topic) %>%
  ggplot(aes(x = factor(topic), y = n, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Number of lines per topic (Biden 2021)",
       x = "Topic",
       y = "Line count") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(axis.title = element_text(size = 10, face = "bold"))

#Topic flow throughout the speech
df_biden_assigned %>%
  ggplot(aes(x = line, y = topic, color = factor(topic))) +
  geom_point(size = 2) +
  labs(title = "Topic flow throughout the speech (Biden 2021)",
       x = "Line number",
       y = "Topic") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(axis.title = element_text(size = 10, face = "bold"))
```

The topic model for President Biden’s 2021 speech reveals five distinct but interconnected themes. One of the key topics centers around policy and economic recovery, shown by terms like “jobs,” “plan,” and “families,” pointing to the administration’s emphasis on employment and pandemic relief. Another dominant topic revolves around legislative urgency and action, with words like “pass,” “congress,” and “right,” suggesting calls for cooperation in moving forward with bills. Additionally, “democracy,” “justice,” and “opportunity” indicate a broader theme around civil values and rebuilding trust in American institutions. Looking at the topic flow, we can see that the distribution of themes is relatively balanced, with each topic receiving significant attention throughout the speech. This suggests Biden aimed for an even rhetorical structure, frequently looping back to reinforce ideas rather than dividing the speech into rigid thematic blocks. The speech opens and closes with references to unity, democracy, and national resilience—helping to tie the more technical sections together with an overarching narrative about hope and collective responsibility.

Overall, thematic analysis of Biden’s 2021 address shows a speech grounded in resilience, recovery, and unity. The five topics identified reflect core priorities of the early Biden administration: economic renewal, legislative action, pandemic response, democratic values, and civic responsibility. These themes are interwoven throughout the speech, with none of them dominating excessively. This suggests a deliberate strategy to present a balanced narrative, where policy details are consistently linked to broader ideals. The topic flow shows that rather than separating ideas into distinct segments, Biden returns to key themes several times, reinforcing his message of rebuilding and national progress.

#### Speech comparison

We will now create a thematic comparison between the four speeches (Trump 2017, Trump 2025, Obama 2013 and Biden 2021) using a single LDA model. We will first unite all the texts in one dataframe.

```{r}
# Create dataframes
df_trump2017 <- tibble(line = 1:length(final_text_2017), text = final_text_2017, president = "Trump 2017")
df_trump2025 <- tibble(line = 1:length(final_text_2025), text = final_text_2025, president = "Trump 2025")
df_obama2013 <- tibble(line = 1:length(final_text_2013), text = final_text_2013, president = "Obama 2013")
df_biden2021 <- tibble(line = 1:length(final_text_2021), text = final_text_2021, president = "Biden 2021")

# Unite in a single dataframe
df_all <- bind_rows(df_trump2017, df_trump2025, df_obama2013, df_biden2021) %>%
  mutate(document = paste(president, line, sep = "_"))  # unique ID by line
```

We will apply tokenization and cleaning.

```{r}
# Personalized stopwords
general_stopwords <- c("it’s", "we’re", "i’ve", "that’s", "let’s", "one", "just", "can", "said", "know", "ever", "get", "now", "also", "percent", "act", "america", "american")

df_words_all <- df_all %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% general_stopwords) %>%
  anti_join(get_stopwords()) %>%
  filter(!str_detect(word, "^[0-9]+$"))
```

We will proceed by creating a DTM and applying LDA.

```{r}
# DTM
df_dtm_all <- df_words_all %>%
  count(document, word) %>%
  cast_dtm(document = document, term = word, value = n)

# LDA
lda_all <- LDA(df_dtm_all, k = 5, control = list(seed = 1234))
```

And we can visualize it too.

```{r}
# Extract gamma
gamma_all <- tidy(lda_all, matrix = "gamma") %>%
  separate(document, into = c("president", "line"), sep = "_")

# Calculate average ratio of each subject per president
gamma_summary <- gamma_all %>%
  group_by(president, topic) %>%
  summarise(mean_gamma = mean(gamma), .groups = "drop")

# Visualization
ggplot(gamma_summary, aes(x = president, y = mean_gamma, fill = factor(topic))) +
  geom_col(position = "fill") +
  labs(title = "Topic Distribution by President",
       x = "President",
       y = "Proportion of Topics",
       fill = "Topic") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

The stacked bar chart shows how each president emphasized the five discovered topics in their speeches. Biden’s 2021 address displays the most balanced distribution among all five topics, with Topic 1 (possibly international affairs and future vision) being slightly more dominant. Obama’s 2013 speech stands out for its stronger focus on Topic 3, which may relate to job creation, economic recovery, or education. Meanwhile, Trump’s 2017 and 2025 speeches share very similar proportions, though the 2025 speech places noticeably more weight on Topic 5, potentially reflecting themes like national identity, patriotism, or calls for unity. Interestingly, Topic 4, which could involve policy initiatives or legislative action, seems more present in Trump’s 2017 speech than in Obama’s or Biden’s. This might highlight how Trump used his first address to stress concrete policy rollouts.

We can also try to make a comparison between the 4 speeches with:

```{r}
# Numerical summary of how much each topic appears across each speech
topic_summary <- gamma_all %>%
  group_by(president, topic) %>%
  summarise(avg_gamma = mean(gamma)) %>%
  arrange(president, topic)

print(topic_summary)

# Spread topic proportions by president
topic_matrix <- topic_summary %>%
  tidyr::pivot_wider(names_from = topic, values_from = avg_gamma, values_fill = 0) %>%
  column_to_rownames("president")

# Compute cosine similarity
similarity_matrix <- as.matrix(dist(topic_matrix, method = "cosine"))
print(round(1 - similarity_matrix, 2))  # 1 - distance = similarity
```

The average gamma values indicate how dominant each topic is within a president's speech. Biden's 2021 speech shows a fairly balanced distribution, with Topic 1 (possibly about future-oriented themes and national identity) being slightly more dominant, followed by Topics 3 and 4. Obama’s 2013 speech has a clear emphasis on Topic 3, which could relate to jobs and economic recovery, suggesting a stronger thematic focus on domestic policy. Trump’s 2017 speech shows Topic 4 as the most dominant, while his 2025 speech places the greatest weight on Topic 5 — hinting at a shift in focus from economic or institutional matters (2017) to perhaps more emotional or populist themes (2025).

The cosine similarity matrix tells us how similar the overall topic proportions are between the four speeches. The highest similarity (0.98) is between Biden 2021 and Trump 2017, which is surprising and suggests a shared focus on similar topics; possibly due to overlapping concerns like recovery, unity, or economic plans. Trump 2025 shows strong similarity to Trump 2017 (0.95), indicating internal thematic consistency. Obama 2013 is slightly less similar to all others (values between 0.87 and 0.95), confirming that his speech emphasized different thematic priorities.

Despite being from different parties and eras, all four speeches share a foundational structure; touching on governance, economic concerns, and national identity. However, subtle shifts can be detected: Obama’s speech leans more heavily into economic rebuilding and policy implementation, while Trump’s speeches evolve from institutional goals to more emotionally driven appeals. Biden’s position seems to blend these elements, aiming for a unifying message that echoes both continuity and reform.

We can also make a comparison with the betas to compare which words are most associated with each topic for each president:

```{r}
# Extract beta for each model
beta_biden <- tidy(lda_biden, matrix = "beta") %>% mutate(president = "Biden 2021")
beta_obama <- tidy(lda_obama, matrix = "beta") %>% mutate(president = "Obama 2013")
beta_trump2017 <- tidy(lda_model_refined, matrix = "beta") %>% mutate(president = "Trump 2017")
beta_trump2025 <- tidy(lda_2025_clean, matrix = "beta") %>% mutate(president = "Trump 2025")

# Combine all into a single table
beta_all <- bind_rows(beta_biden, beta_obama, beta_trump2017, beta_trump2025)

top_terms_by_president <- beta_all %>%  
  group_by(president, topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ggplot(top_terms_by_president, aes(x = reorder(term, beta), y = beta, fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ president + topic, scales = "free", ncol = 5) +
  coord_flip() +
  labs(title = "Top Terms per Topic and President",
       x = "Term", y = "Beta") +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 10, face = "bold"),
    strip.text = element_text(size = 10, face = "bold"))
```

Topic 1 appears to focus on temporal or national framing (words like “years,” “world,” “century,” “energy,” “country”). Biden and Obama use temporal markers like “century” and “years” more heavily, whereas Trump 2017 emphasizes “people” and “country,” and Trump 2025 introduces less semantically rich terms like “tonight” and “members.” This suggests that Biden and Obama frame their discourse within broader historical or national narratives, while Trump’s tone is more anchored in people and immediacy. Topic 2 emphasizes legislative or executive action; terms like “jobs,” “act,” “congress,” “plan,” “president.” Biden leads with “jobs” and “plan,” showing a proactive, policy-driven agenda. Obama includes “congress,” “can,” and “tonight,” indicating direct appeals for action. Trump 2017 uses “today,” “world,” and “nation” to ground his points in current national challenges. Trump 2025's focus shifts to adjectives like “good” and “beautiful,” reflecting a more rhetorical or persuasive tone rather than action-based content.

Topic 3 contains terms related to economic or systemic issues (“work,” “tax,” “system,” “country,” “nation,” “immigration”). Obama emphasizes economic concerns (“jobs,” “high,” “tax”), while Biden highlights social policy (“families,” “care,” and “plan”). Trump 2017’s emphasis on “immigration,” “system,” and “access” aligns with his strong rhetoric on national security and structural change. Trump 2025 is more subdued, with “million” and “people” suggesting a populist or budget-driven framing. Topic 4 seems linked to governance, rights, and institutions; words like “congress,” “pass,” “right,” “pandemic,” “department,” “fight,” etc. Biden's top terms like “congress,” “pass,” and “americans” signal a legislative focus with civic responsibility. Obama focuses more on urgency and moral reasoning (“need,” “get,” “make”), while Trump 2017 features “department,” “country,” and “terrorists,” suggesting a national security tone. Trump 2025 emphasizes action and resistance with “fight,” “make,” “ever,” and “american.” Topic 5 is rooted in democratic values and collective identity; with “democracy,” “people,” “justice,” “lives,” “citizens,” “families,” “vote.” Biden uses strong democratic markers like “democracy,” “justice,” “opportunity.” Obama emphasizes “vote,” “families,” “deserve” — reinforcing civic engagement. Trump 2017's terms are less emotionally charged (“across,” “today,” “nation”), and Trump 2025 again returns to general patriotic terms like “country,” “us,” and “ever.” This shows how Trump leans more on nationalism than explicitly democratic ideals.

#### General topic modeling discussion

Although this comparison can give us clear information differences, we will try to compare each president's speeches in a more separate way. First, when comparing Trump’s 2017 and 2025 speeches, one of the most striking differences lies in the dominant topics and how they are introduced throughout the speech. In 2017, Trump's address focused heavily on national security, immigration, and economic revitalization, with clear references to restoring borders, protecting American workers, and bringing manufacturing back to the U.S. These topics appeared in a structured pattern throughout the speech, giving a sense of control and alignment with his "America First" platform. By contrast, the 2025 speech, while still retaining a populist tone, appeared slightly more fragmented, with greater emphasis on broad themes such as national identity, citizen stories, and social appeals. This shift suggests a subtle change in rhetorical strategy (from implementation of promises to reflecting on outcomes and reinforcing emotional appeal).

In terms of topic flow, the 2017 speech demonstrated more distinct topic blocks, with each section clearly dedicated to a different policy domain, including healthcare, military, and economic policy. The 2025 speech, on the other hand, showed a more combined flow of topics, often mixing references to personal stories, political achievements, and calls for unity within the same segments. This may indicate an attempt to appeal to a broader audience or to reflect a matured presidential persona that integrates policies within a larger national narrative. Another noteworthy point is the emotional tone. Trump’s 2017 speech, while firm and assertive, maintained a business-like delivery that aligned with his image as an outsider entering politics to "clean up Washington." The language in the 2025 speech, however, leaned more heavily into emotional and patriotic appeals, with a notable increase in references to individuals, values, and legacy. This emotional shift could be interpreted as an effort to consolidate public support and present his leadership in a more reflective, almost legacy-defining light. Finally, when looking at the vocabulary and keywords driving the topic models, we observe that terms in the 2017 speech were more policy-oriented ("jobs," "tax," "border," "security"), whereas the 2025 topics included words like "people," "freedom," "back," and "great," suggesting a move toward symbolism and shared identity rather than just concrete legislative goals. This linguistic change further supports the idea that Trump’s later speech pivoted from promises and plans to affirming a collective narrative, possibly to shape public memory of his administration.

Second, when comparing Trump with Obama, the contrast in rhetorical structure and thematic distribution becomes even more pronounced. Obama’s 2013 State of the Union address demonstrated a high degree of topical coherence, with economic recovery and legislative priorities being dominant. His speech emphasized forward-thinking policies (green energy, education reform) and maintained a tone of measured optimism. In contrast, Trump’s speeches, especially in 2017, focused more on disruption and change, using sharper, more action-oriented language to underline a break from previous administrations. This difference highlights a broader ideological contrast: Obama positioned himself as a unifier and reformer, while Trump framed himself as a reformer through confrontation. Another point of difference is how both presidents framed national identity and values. Obama’s discourse often integrated diversity, inclusivity, and opportunity as core American values, whereas Trump centered his discourse around sovereignty, protectionism, and strength. This is evident in the dominant topics from their respective models where Obama’s speech frequently referred to “families,” “education,” “jobs,” and “future,” Trump’s top terms included “border,” “order,” “enforcement,” and “America First.” These contrasting priorities reflect not just different political agendas but also different visions of American society.

Third, when comparing Trump and Biden, the difference lies not only in content but also in tone and topic integration. Biden’s 2021 speech featured a balanced topic flow, with equal attention given to recovery (post-pandemic), democracy, healthcare, and unity. The emotional register was empathetic and often invoked shared struggle and resilience, contrasting with Trump’s assertive and goal-focused tone. Biden's speech made frequent use of collective pronouns ("we," "our") and framed policy as a collaborative national effort, whereas Trump’s speeches often leaned into "I" and "they," underscoring personal leadership and external threats.

Lastly, from a structural perspective, Biden’s and Obama’s speeches show a higher tendency to weave personal stories into policy discussions, using them to illustrate larger political points. Trump, while also invoking citizen stories, tended to use them more as testimonies of government success or political justification rather than vehicles for empathy. This rhetorical contrast is also evident in the distribution and transitions between topics: Biden and Obama frequently revisited themes throughout their speeches to build cohesion, while Trump more often maintained strong thematic clusters, particularly in the 2017 address. This makes Trump’s speeches feel more segmented, while Obama’s and Biden’s come across as narratively fluid.

Overall, the topic modeling analysis across the four presidential speeches highlights how each leader shaped their message to reflect their political priorities and the context of their presidency. Trump’s speeches leaned toward strong national identity, economic protectionism, and bold rhetoric, while Obama emphasized hope, reform, and collective responsibility. Biden's speech blended crisis recovery with unity and democratic values. So, while there are thematic overlaps, each speech reflects a unique rhetorical strategy and vision for the country.

## Conclusion

After analyzing the speeches of Trump (2017 and 2025), Obama (2013), and Biden (2021) through sentiment analysis, term frequency (TF-IDF), and topic modeling, it becomes clear that political language is not merely a vehicle for information but a powerful tool for shaping emotion, identity, and public perception. Each president, through their choice of words, constructs a narrative that reflects not only their policy agenda but also the historical and social moment they are responding to.

In the sentiment analysis phase, we observed notable contrasts. Trump’s 2017 address stood out for its high proportion of negative terms, with repeated references to crisis, danger, and threats—painting a stark portrait of a nation in decline. Obama and Biden, by contrast, employed a more optimistic tone, with positive words centering around unity, hope, and shared purpose. Interestingly, Trump’s 2025 speech, while still grounded in warnings and assertive language, integrated a noticeably higher number of joyful and uplifting words—suggesting a possible rhetorical shift toward a more emotionally engaging or legacy-driven tone.

The frequency and TF-IDF analysis helped spotlight not only commonly used terms but those uniquely representative of each speech. Biden’s emphasis on terms like *pandemic* and *vaccinated* anchors his message in the urgency of post-COVID recovery. Obama’s use of words like *let's* and *encourage* reflects a call for action and cooperation, whereas Trump’s lexical focus in both speeches leaned toward protectionism and security—terms such as *immigrant*, *ukraine*, and *inflation* being particularly prominent.

Topic modeling, finally, allowed us to delve deeper into the structural and rhetorical strategies behind each address. Obama’s and Biden’s speeches were thematically woven, cycling through economic, social, and moral themes in a balanced, fluid manner. Trump’s 2017 speech was more segmented, with each policy area clearly separated, underscoring a methodical presentation of issues. However, the 2025 speech reflected a more blended narrative approach—one where emotion, identity, and national pride are interlaced with policy and political rhetoric. This evolution suggests a shift from outsider urgency to legacy consolidation, where storytelling becomes just as important as action plans.

Altogether, the combination of sentiment, frequency, and topic modeling reveals how presidential speeches are crafted not just to inform, but to resonate. These texts are performative acts—designed to persuade, unify, divide, comfort, or mobilize. While all four presidents share themes like economy, governance, and national identity, the way they frame these ideas differs significantly. Since what politicians say often sticks with people even more than what they actually do, digging into how these speeches are built (what words they use and what topics they focus on) really helps us see what mattered to people at the time, what they were worried about, and what they were hoping for.
